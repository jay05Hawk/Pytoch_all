{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM2TrYCIeebM1XGNifbT+9/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jay05Hawk/Pytorch_all/blob/main/Pytorch_1%2C0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rhp2yDgGWPqv"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is PyTorch?\n",
        "\n",
        "PyTorch is a Python-based scientific computing package that uses the power of graphics processing units(GPU). It is also one of the preferred deep learning research platforms built to provide maximum flexibility and speed. It is known for providing two of the most high-level features; namely, tensor computations with strong GPU acceleration support and building deep neural networks on a tape-based autograd systems.\n",
        "\n",
        "There are many existing Python libraries which have the potential to change how deep learning and artificial intelligence are performed, and this is one such library. One of the key reasons behind PyTorch’s success is it is completely Pythonic and one can build neural network models effortlessly. It is still a young player when compared to its other competitors, however, it is gaining momentum fast."
      ],
      "metadata": {
        "id": "y1ZOB8hf1voZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Brief History about PyTorch\n",
        "\n",
        "Since its release in January 2016, many researchers have continued to increasingly adopt PyTorch. It has quickly become a go-to library because of its ease in building extremely complex neural networks. It is giving a tough competition to TensorFlow especially when used for research work. However, there is still some time before it is adopted by the masses due to its still “new” and “under construction” tags.\n",
        "\n",
        "PyTorch creators envisioned this library to be highly imperative which can allow them to run all the numerical computations quickly. This is an ideal methodology which fits perfectly with the Python programming style. It has allowed deep learning scientists, machine learning developers, and neural network debuggers to run and test part of the code in real time. Thus they don’t have to wait for the entire code to be executed to check whether it works or not.\n",
        "You can always use your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch functionalities and services when required. Now you might ask, why PyTorch? What’ so special in using it to build deep learning models?\n",
        "\n",
        "The answer is quite simple, PyTorch is a dynamic library (very flexible and you can use as per your requirements and changes) which is currently adopted by many of the researchers, students, and artificial intelligence developers. In the recent Kaggle competition, PyTorch library was used by nearly all of the top 10 finishers.\n",
        "\n",
        "Some of the key highlights of PyTorch includes:\n",
        "\n",
        "__Simple Interface:__ It offers easy to use API, thus it is very simple to operate and run like Python.\n",
        "\n",
        "__Pythonic in nature:__ This library, being Pythonic, smoothly integrates with the Python data science stack. Thus it can leverage all the services and functionalities offered by the Python environment.\n",
        "\n",
        "__Computational graphs:__ In addition to this, PyTorch provides an excellent platform which offers dynamic computational graphs, thus you can change them during runtime. This is highly useful when you have no idea how much memory will be required for creating a neural network model."
      ],
      "metadata": {
        "id": "rIcZtH0k1wAY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <mark style=\"\">__Tensors__</mark>\n",
        "\n",
        "Tensor is similar to <mark style=\"background-color: Green\">__Numpy's n-darray__</mark>, the additional point for Tensors in we can use it in GPUs to accelerate computing."
      ],
      "metadata": {
        "id": "ZH2ED00E98pm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWzFkaD4WS7Z"
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Note:__ Uninitialized matrix is declared, but doesn't contain definite known values before it is used. When we created an Unintialized matrix, whatever values were allocated inside the memory will apear as the initial values.\n",
        "\n",
        "$\\color{blue}{\\text{Construct 6x3 matrix, uninitialized:}}$"
      ],
      "metadata": {
        "id": "NDPLOZt4-5HN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hYlNyBDWTHc"
      },
      "source": [
        "a = torch.empty(6,3)\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\color{green}{\\text{Construct a randomly initialized matrix:}}$"
      ],
      "metadata": {
        "id": "qgkc3eX4ATHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.rand(4,3)\n",
        "print(a)"
      ],
      "metadata": {
        "id": "Fj0KWChBAE0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\color{green}{\\text{Construct a matrix filled zeros and of dtype long:}}$"
      ],
      "metadata": {
        "id": "Y1oHPOL1Aqkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.zeros(4,3, dtype=torch.long)\n",
        "print(a)"
      ],
      "metadata": {
        "id": "tBEy728EAIg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Construct a tensor with data:\n",
        "a = torch.tensor([7.8, 5])\n",
        "type(a)"
      ],
      "metadata": {
        "id": "de7ZrGmgAH5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "or we can create new tensor with existing tensor.These methods we reuse its properties of input tensor, e.g. dtype, unless new values are provided by us."
      ],
      "metadata": {
        "id": "Wnt8rygJBPxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = a.new_ones(6,5, dtype=torch.double)    # new methods take in sizes\n",
        "print(a)\n",
        "\n",
        "a = torch.randn_like(a, dtype=torch.float)  # override dtype\n",
        "print(a)                                    # result will be the same size\n",
        "\n",
        "print(a.size())"
      ],
      "metadata": {
        "id": "7vGKYgG5AH2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: <mark style=\"background-color: Yellow\">torch_size</mark> is actually a tuple, so it supports all tuple operations."
      ],
      "metadata": {
        "id": "lmdX98rgB2-6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Operations\n",
        "\n",
        "There are multiple syntaxes for operations. In the following examples, we used addition operation,\n"
      ],
      "metadata": {
        "id": "EAPpTWlICBC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Addition: syntax1\n",
        "b = torch.rand(6,5)\n",
        "print(a + b)"
      ],
      "metadata": {
        "id": "2_w3jccmAHzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Addition: syntax2\n",
        "print(torch.add(a,b))"
      ],
      "metadata": {
        "id": "bF99C3BvAHw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Addition: providing an output as an argument\n",
        "result = torch.empty(6, 5)\n",
        "torch.add(a, b, out=result)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "ksMlWWvNAHt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Addition: in place\n",
        "\n",
        "# adds a to b\n",
        "b.add_(a)                 #here is same thing the value store in b.      first char store value\n",
        "print(b)"
      ],
      "metadata": {
        "id": "EeitTrj0AHrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Note:__ Any operation that mutates a  tensor in-place is post-fixed with an <mark style=\"highlight: red\">_.</mark> For example: a.copy_(b), a.b_(), will change a."
      ],
      "metadata": {
        "id": "GJnl3AhSDKdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(a[:,2])"
      ],
      "metadata": {
        "id": "Ytj0_zXfAHoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Resizing:__ We can resize or reshape tensor, use <mark style=\"background-color: Yellow\">tensor.view</mark> for that:"
      ],
      "metadata": {
        "id": "B4_Chm3hEgVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a.view(9)"
      ],
      "metadata": {
        "id": "mYLiGZpCEzsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.randn(3, 3)\n",
        "b = a.view(9) # shows all elements in metice\n",
        "c = a.view(-1, 9)  # the size -1 is inferred from other dimensions\n",
        "print(a.size(), b.size(), c.size())"
      ],
      "metadata": {
        "id": "htXGy2fnAHlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have one value tensor, use <mark style=\"background-color: Yellow\">.item()</mark> to get the value of the Python number\n"
      ],
      "metadata": {
        "id": "1I_O49M7FVib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.randn(1)\n",
        "print(a)\n",
        "print(a.item())"
      ],
      "metadata": {
        "id": "4sB6JJUoAHip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NumPy Bridge\n",
        "\n",
        "Converting a Torch Tensor to NumPy array and vice versa is breeze.\n",
        "\n",
        "The Torch Tensor and NumPy array will share their underlying memory locations (if the Torch Tensor is on CPU), and changing one will change the other."
      ],
      "metadata": {
        "id": "EgE2IJovFjXz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Converting a Torch tensor to NumPy array__"
      ],
      "metadata": {
        "id": "VQifjOFEFsrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.ones(4)\n",
        "print(x)"
      ],
      "metadata": {
        "id": "XuiCMly2AHfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = x.numpy()\n",
        "print(y)"
      ],
      "metadata": {
        "id": "w9_J8Km9AHcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__See how numpy array changed in value__"
      ],
      "metadata": {
        "id": "UsON1fZjF-6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.add_(1)\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "jQAKiZ6YAHYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Converting NumPy array to Torch tensor__\n",
        "\n",
        "lets see how changing the numpy array changed the Torch Tensor automatically"
      ],
      "metadata": {
        "id": "3hr12GROGReZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "f = np.ones(4)\n",
        "g = torch.from_numpy(f)\n",
        "np.add(f, 1, out=f)\n",
        "print(f)\n",
        "print(g)"
      ],
      "metadata": {
        "id": "b1biGINHAHVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the Tensors on the CPU except a CharTensor support converting to NumPy and back.\n",
        "\n",
        "$\\color{blue}{\\text{************CUDA Tensors**********}}$\n",
        "\n",
        "Tensors can be moved onto any device using the <mark style=\"background-color: Yellow\">.to</mark> method."
      ],
      "metadata": {
        "id": "IVdaoDtOGn-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let us run this cell only if CUDA is available  --->GPU\n",
        "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")          # a CUDA device object\n",
        "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
        "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
        "    z = x + y\n",
        "    print(z)\n",
        "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!\n"
      ],
      "metadata": {
        "id": "734uN_3fAHSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  $\\color{blue}{\\text{AUTOGRAD : }}$ Automatic Differentiaition\n",
        "\n",
        "- This class is an engine to calculate derivatives. It records the graph of all the operations performed on a gradient      enabled tensor and creates a acyclic graph called the dynamic computational graph(DCG).The leaves of this graph are input tensors and the roots are output tensors. Gradients are calculated by tracing the graph from the root to the leaf and multiplying every gradient in the way using the chain rule."
      ],
      "metadata": {
        "id": "Zv8Ntz6cHn0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensor\n",
        "\n",
        "<mark style=\"background-color: Yellow\">torch.Tensor</mark> is the central class of the package.If we set its attribute <mark style=\"background-color: Yellow\">.requires_grad</mark> as <mark style=\"background-color: dark grey\">True</mark>, it starts to track all operations on it. When you finish your computation you can call <mark style=\"background-color: Yellow\">.backward()</mark> and have all the gradients computed automatically. The gradient of this tensor will be accumulated into <mark style=\"background-color: Yellow\">.grad</mark> attribute."
      ],
      "metadata": {
        "id": "HeLfjIOFINj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To stop a tensor from tracking history, you can call <mark style=\"background-color: Yellow\">.detach()</mark> to detach it from the computation history, and to prevent future computation from being tracked.\n",
        "\n",
        "\n",
        "To prevent tracking history(and using memory), you can also wrap the code block in with <mark style=\"background-color: Yellow\">torch.no_grad():</mark>. This can be particularly helpful when evaluating a model because the model may have trainable parameters with <mark style=\"background-color: Yellow\">requires_grad=True</mark>, but for which we don’t need the gradients.\n",
        "\n",
        "There's one more class which is very important in autograd implementation - a <mark style=\"background-color: Yellow\">Function</mark>\n",
        "\n",
        "<mark style=\"background-color: Yellow\">Tensor</mark> and <mark style=\"background-color: Yellow\">Function</mark> are interconnected and build up an acyclic graph, that encodes a complete history of computation. Each tensor has a <mark style=\"background-color: Yellow\">.grad_fn</mark> attribute that references a Function that has created the Tensor (except for Tensors created by the user - their <mark style=\"background-color: Yellow\">grad_fn is None</mark>).\n",
        "\n",
        "If you want to compute the derivatives, you can call <mark style=\"background-color: Yellow\">.backward()</mark> on a <mark style=\"background-color: Yellow\">Tensor</mark>. If <mark style=\"background-color: Yellow\">Tensor</mark> is a scalar (i.e. it holds a one element data), you don’t need to specify any arguments to <mark style=\"background-color: Yellow\">backward()</mark>, however if it has more elements, you need to specify a <mark style=\"background-color: Yellow\">gradient</mark> argument that is a tensor of matching shape."
      ],
      "metadata": {
        "id": "lmQmszzkIT5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "bhMq4g2yIiXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a tensor and set $\\color{green}{\\text{requires_grad=True}}$ to track computation with it."
      ],
      "metadata": {
        "id": "TeLomzMUIhCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.ones(2, 2, requires_grad=True)\n",
        "print(a)"
      ],
      "metadata": {
        "id": "pCBW2FlCAHQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Do a tensor operation:\n",
        "b = a + 2\n",
        "print(b)"
      ],
      "metadata": {
        "id": "gwZYDlg6AFIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#b was created as a result of an operatio, so it has a <mark style=\"background-color: Yellow\">grad_fn</mark>\n",
        "print(b.grad_fn)"
      ],
      "metadata": {
        "id": "GxJp4MwmAFUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = b * b * 3\n",
        "out = c.mean()\n",
        "\n",
        "print(c, out)"
      ],
      "metadata": {
        "id": "5Yt5CWrCAFdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<mark style=\"background-color: Yellow\">.requires_grad_( ... )</mark>  changes an existing Tensor’s <mark style=\"background-color: Yellow\">requires_grad</mark> flag in-place. The input flag defaults to False if not given."
      ],
      "metadata": {
        "id": "iq-A7j2gJicV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p = torch.randn(3, 3)\n",
        "p = ((p * 3) / (p - 1))\n",
        "print(p.requires_grad)\n",
        "p.requires_grad_(True)\n",
        "print(p.requires_grad)\n",
        "q = (p * p).sum()\n",
        "print(q.grad_fn)"
      ],
      "metadata": {
        "id": "Lp0YWHcQJZ9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradients\n",
        "\n",
        "Let's backdrop now. Because <mark style=\"background-color: magenta\">out</mark> contains a single scalar, <mark style=\"background-color: Yellow\">out.backword</mark> is equivalent to  <mark style=\"background-color: Yellow\">out.backward(torch.tensor(1.))</mark>. "
      ],
      "metadata": {
        "id": "ajesG7wgJtGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out.backward()"
      ],
      "metadata": {
        "id": "Sj8bBeXnJayg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print gradients d(out)/dx\n",
        "print(a.grad)"
      ],
      "metadata": {
        "id": "8zA7DfKdJcVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should have got a matrix of 4.5. Let’s call the out Tensor “o”. We have that \n",
        "\n",
        "$o= \\frac{1}{4} \\sum c_i $ $c_i= 3(a_i+2)^2$  and $c_i|_{a_i=1}= 27$ . \n",
        "\n",
        "Therefore, $\\frac{\\partial_0}{\\partial a_i}=\\frac{3(x_i+2)}{2}$, \n",
        "\n",
        "hence\n",
        "$\\frac{\\partial_0}{\\partial a_i}|_{a_i=1} = \\frac{9}{2} = 4.5$\n",
        "\n"
      ],
      "metadata": {
        "id": "dxqUujL1J9xl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Now let's have a look at an example of vector-Jacobian product:\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "\n",
        "y = x * 2\n",
        "while y.data.norm() < 1000:\n",
        "    y = y * 2\n",
        "\n",
        "print(y)"
      ],
      "metadata": {
        "id": "sXIc0JwUJcJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now in this case y is no longer a scalar.<mark style=\"background-color: Yellow\">torch.autograd(torch.tensor(1.))</mark> could not compute the full Jacobian directly, but if we just want the vector-Jacobian product, simply pass the vector to backward as argument:"
      ],
      "metadata": {
        "id": "eCw9Ct0EKSz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
        "y.backward(v)\n",
        "\n",
        "print(x.grad)"
      ],
      "metadata": {
        "id": "1OVDrMjTJcGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also stop autograd from tracking history on Tensors with <mark style=\"background-color: Yellow\">.requires_grad=True</mark> either by wrapping the code block in with <mark style=\"background-color: Yellow\">torch.no_grad()</mark>:"
      ],
      "metadata": {
        "id": "L9AhnMguKd88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.requires_grad)\n",
        "print((x ** 2).requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "    print((x ** 2).requires_grad)"
      ],
      "metadata": {
        "id": "EfZ0WaCDJbvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Or by using .detach() to get a new Tensor with the same content but that does not require gradients:\n",
        "print(x.requires_grad)\n",
        "y = x.detach()\n",
        "print(y.requires_grad)\n",
        "print(x.eq(y).all())"
      ],
      "metadata": {
        "id": "OaMHj7cOJbr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Networks\n",
        "\n",
        "We can construct neural networks using the <mark style=\"background-color: Yellow\">torch.nn</mark> package.\n",
        "\n",
        "Now that you had a glimpse of autograd, nn depends on autograd to define models and differentiate them. An nn.Module contains layers, and a method forward(input)that returns the output.\n",
        "\n",
        "For example, look at this network that classifies digit images:\n",
        "<img src=\"neural.png\"> \n",
        "\n",
        "\n",
        "### convnet\n",
        "\n",
        "It is a simple feed-forward network. It takes the input,feeds it through several layers one after the other, and then finally gives the output.\n",
        "\n",
        "A typical training procedure for a neural network is as follows:\n",
        "- Define the neural network that has some learnable parameters (or weights)\n",
        "- Iterate over a dataset of inputs\n",
        "- Process input through the network\n",
        "- Compute the loss (how far is the output from being correct)\n",
        "- Propagate gradients back into the network’s parameters\n",
        "- Update the weights of the network, typically using a simple update rule: <mark style=\"background-color: light-blue\">weight = weight - learning_rate * gradient</mark>\n",
        "        \n",
        "### Define the network\n",
        "\n",
        "Let's define the network:"
      ],
      "metadata": {
        "id": "jx_smgKNK0Cx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class network(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(network, self).__init__()\n",
        "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
        "        # kernel\n",
        "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
        "        # an affine operation: y = mx + b\n",
        "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Max pooling over a (2, 2) window\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        # If the size is a square you can only specify a single number\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "\n",
        "net = network()\n",
        "print(net)"
      ],
      "metadata": {
        "id": "ue7uQkR8JboB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You just have to define the <mark style=\"background-color: yellow\">forward</mark>,  and the <mark style=\"background-color: yellow\">backward</mark> function (where gradients are computed) is automatically defined for you using autograd. You can use any of the Tensor operations in the <mark style=\"background-color: yellow\">forward</mark> function.\n",
        "\n",
        "The learnable parameters of a model are returned by <mark style=\"background-color: yellow\">net.parameters()</mark>."
      ],
      "metadata": {
        "id": "pSNJdXzXLPk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = list(net.parameters())"
      ],
      "metadata": {
        "id": "vhIQQFfqMXW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(s)"
      ],
      "metadata": {
        "id": "2fPMfXLaJblG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s[0].size()"
      ],
      "metadata": {
        "id": "SQzy4XHGMnz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s[1].size()"
      ],
      "metadata": {
        "id": "XNaZ3hKOMnwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s[2].size()"
      ],
      "metadata": {
        "id": "yEJC3mzKMntG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s[3].size()"
      ],
      "metadata": {
        "id": "L0No_RpMMnpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s[4].size()"
      ],
      "metadata": {
        "id": "rhNXAuJdJbhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s[5].size()"
      ],
      "metadata": {
        "id": "kcopaPktJbe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s[6].size()"
      ],
      "metadata": {
        "id": "dXSdETnIJa32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s[7].size()"
      ],
      "metadata": {
        "id": "61kHpCOcM0lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s[8].size()"
      ],
      "metadata": {
        "id": "vXdNNanuJa9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s[9].size()"
      ],
      "metadata": {
        "id": "F24ciII1M2fE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = list(net.parameters())\n",
        "print(len(params))\n",
        "print(params[0].size())  # conv1's .weight"
      ],
      "metadata": {
        "id": "4VVUE9QzM5cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s try a random 32x32 input. Note: expected input size of this net (LeNet) is 32x32. To use this net on the MNIST dataset, please resize the images from the dataset to 32x32."
      ],
      "metadata": {
        "id": "ohSHQzuGNfUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inp = torch.randn(1, 1, 32, 32)\n",
        "out = net(inp)\n",
        "print(out)"
      ],
      "metadata": {
        "id": "EpgOa3F0M5Y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zero the gradient buffers of all parameters and backprops with random gradients:"
      ],
      "metadata": {
        "id": "9T-9SzXkNodf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net.zero_grad()\n",
        "out.backward(torch.randn(1, 10))"
      ],
      "metadata": {
        "id": "2iE3fnQKM5Rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Note:__\n",
        "<mark style=\"background-color: yellow\">torch.nn</mark> only supports mini-batches. The entire <mark style=\"background-color: yellow\">torch.nn</mark> package only supports inputs that are a mini-batch of samples, and not a single sample.\n",
        "For example, <mark style=\"background-color: yellow\">nn.Conv2d</mark> will take in a 4D Tensor of <mark style=\"background-color: yellow\">nSamples x nChannels x Height x Width</mark>.\n",
        "If you have a single sample, just use <mark style=\"background-color: yellow\">input.unsqueeze(0)</mark> to add a fake batch dimension."
      ],
      "metadata": {
        "id": "TvePC1yUN5YB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Function\n",
        "\n",
        "A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target.\n",
        "There are several different loss functions under the nn package . A simple loss is: <mark style=\"background-color: yellow\">nn.MSELoss</mark> which computes the mean-squared error between the input and the target.\n",
        "    \n",
        "For example:"
      ],
      "metadata": {
        "id": "NLAMvgzON9Zj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = net(inp)\n",
        "target = torch.randn(10)  # a dummy target, for example\n",
        "target = target.view(1, -1)  # make it the same shape as output\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "loss = criterion(output, target)\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "iipHacrPM5JA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, if you follow loss in the backward direction, using its <mark style=\"background-color: yellow\">.grad_fn</mark>attribute, you will see a graph of computations that looks like this:"
      ],
      "metadata": {
        "id": "RLZ8bMAuOG6T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "input  ->   conv2d  ->   relu  ->   maxpool2d  ->   conv2d ->   relu  ->   maxpool2d\n",
        "      \n",
        "      -> view -> linear -> relu -> linear -> relu -> linear\n",
        "      \n",
        "      -> MSELoss\n",
        "      \n",
        "      -> loss"
      ],
      "metadata": {
        "id": "W35DUGr8OIWa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, when we call <mark style=\"background-color: yellow\">loss.backward()</mark>, the whole graph is differentiated w.r.t. the loss, and all Tensors in the graph that has requires_grad=True will have their .grad Tensor accumulated with the gradient.\n",
        "\n",
        "\n",
        "For illustration, let us follow a few steps <mark style=\"background-color: yellow\">backward</mark>:"
      ],
      "metadata": {
        "id": "JM2DSKZLOPa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss.grad_fn)  # MSELoss\n",
        "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
        "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
      ],
      "metadata": {
        "id": "EEFFWXFDM5Fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backprop\n",
        "\n",
        "To backpropagate the error all we have to do is to <mark style=\"background-color: yellow\">loss.backward()</mark>. You need to clear the existing gradients though, else gradients will be accumulated to existing gradients.\n",
        "\n",
        "\n",
        "Now we shall call <mark style=\"background-color: yellow\">loss.backward()</mark>, and have a look at conv1’s bias gradients before and after the backward."
      ],
      "metadata": {
        "id": "CFzTQS36OX6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
        "\n",
        "print('conv1.bias.grad before backward')\n",
        "print(net.conv1.bias.grad)\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print('conv1.bias.grad after backward')\n",
        "print(net.conv1.bias.grad)"
      ],
      "metadata": {
        "id": "5S6FyrUjM5Ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Update the weights\n",
        "\n",
        "The simplest update rule used in practice is the Stochastic Gradient Descent (SGD):\n",
        "   \n",
        " $\\color{blue}{\\text{ weight = weight - learning_rate * gradient}}$\n",
        "   \n",
        "We can implement this using simple Python code:"
      ],
      "metadata": {
        "id": "Yc57QKmbOgqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "for f in net.parameters():\n",
        "    f.data.sub_(f.grad.data * learning_rate)"
      ],
      "metadata": {
        "id": "TpltlPuEM46G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, as you use neural networks, you want to use various different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc. To enable this, we built a small package: torch.optim that implements all these methods. Using it is very simple:"
      ],
      "metadata": {
        "id": "xwGGS5iSO3_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# create your optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "# in your training loop:\n",
        "optimizer.zero_grad()   # zero the gradient buffers\n",
        "output = net(inp)\n",
        "loss = criterion(output, target)\n",
        "loss.backward()\n",
        "optimizer.step()    # Does the update"
      ],
      "metadata": {
        "id": "97ReuzzmM417"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe how gradient buffers had to be manually set to zero using <mark style=\"background-color:yellow\">optimizer.zero_grad()</mark>. This is because gradients are accumulated as explained in the <mark style=\"font-color:red\">Backprop</mark> section."
      ],
      "metadata": {
        "id": "TM5SYNCMPAoe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "939_NMzrM4E7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}